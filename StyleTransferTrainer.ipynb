{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install the [fastai](https://docs.fast.ai/) Library\n\nFirst, we'll install the fastai library which is built on top of PyTorch. We'll be using pure PyTorch for training the model but the fastai library includes some convenience functions that we'll use to download the training dataset.","metadata":{"id":"69bYigdcg90A"}},{"cell_type":"code","source":"!pip install fastai==2.2.5","metadata":{"id":"-Q83PMa6b-RU","executionInfo":{"status":"ok","timestamp":1635763789889,"user_tz":-120,"elapsed":202661,"user":{"displayName":"Augustas Masolas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBl4xvdqAPkl9CWBCfg6hmPeWdBpOaf2NVbWrMgQ=s64","userId":"10648435764537489448"}},"execution":{"iopub.status.busy":"2021-12-01T15:36:00.578892Z","iopub.execute_input":"2021-12-01T15:36:00.579226Z","iopub.status.idle":"2021-12-01T15:36:06.475994Z","shell.execute_reply.started":"2021-12-01T15:36:00.579195Z","shell.execute_reply":"2021-12-01T15:36:06.474923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import Dependencies\nNext, we need to import the required python modules and packages.","metadata":{"id":"KhnbdLSdnIBM"}},{"cell_type":"code","source":"import os\nimport time\nfrom pathlib import Path\n# Tuple-like objects that have named fields\n# https://docs.python.org/3/library/collections.html#collections.namedtuple\nfrom collections import namedtuple\n\n# A convenience function for downloading files from a url to a destination folder\n# https://docs.fast.ai/data.external.html#untar_data\nfrom fastai.data.external import untar_data\n\n# Provides image processing capabilities\n# https://pillow.readthedocs.io/en/stable/reference/Image.html\nfrom PIL import Image\n\nimport torch\n\n# Used to iterate over the dataset during training \n# https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\nfrom torch.utils.data import DataLoader\n\n# Contains definitions of models. We'll be downloading a pretrained VGG-19 model\n# to judge the performance of our style transfer model.\n# https://pytorch.org/vision/stable/models.html#torchvision.models.vgg19\nfrom torchvision.models import vgg19\n# Common image transforms that we'll use to process images before feeding them to the models\n# https://pytorch.org/vision/stable/transforms.html\nfrom torchvision import transforms\n# Loads images from a directory and applies the specified transforms\n# https://pytorch.org/vision/stable/datasets.html#imagefolder\nfrom torchvision.datasets import ImageFolder","metadata":{"id":"yFbPJHlZcQHw","executionInfo":{"status":"ok","timestamp":1635763791190,"user_tz":-120,"elapsed":1315,"user":{"displayName":"Augustas Masolas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBl4xvdqAPkl9CWBCfg6hmPeWdBpOaf2NVbWrMgQ=s64","userId":"10648435764537489448"}},"execution":{"iopub.status.busy":"2021-12-01T15:36:06.477791Z","iopub.execute_input":"2021-12-01T15:36:06.478142Z","iopub.status.idle":"2021-12-01T15:36:06.487666Z","shell.execute_reply.started":"2021-12-01T15:36:06.478099Z","shell.execute_reply":"2021-12-01T15:36:06.486743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utility Functions\n\nWe'll define some utility functions for making new directories, loading and saving images, and stylizing images using model checkpoints.","metadata":{"id":"EP8IuH98oT14"}},{"cell_type":"code","source":"def make_dir(dir_name: str):\n    \"\"\"Create the specified directory if it doesn't already exist\"\"\"\n    dir_path = Path(dir_name)\n    try:\n        dir_path.mkdir()\n    except:\n        print(\"Directory already exists.\")\n\ndef load_image(filename: str, size: int=None, scale: float=None):\n    \"\"\"Load the specified image and return it as a PIL Image\"\"\"\n    img = Image.open(filename)\n    if size is not None:\n        img = img.resize((size, size), Image.ANTIALIAS)\n    elif scale is not None:\n        img = img.resize((int(img.size[0] / scale), int(img.size[1] / scale)), Image.ANTIALIAS)\n    return img\n\ndef save_image(filename: str, data: torch.Tensor):\n    \"\"\"Save the Tensor data to an image file\"\"\"\n    img = data.clone().clamp(0, 255).numpy()\n    img = img.transpose(1, 2, 0).astype(\"uint8\")\n    img = Image.fromarray(img)\n    img.save(filename)\n\ndef load_checkpoint(model_path):\n    state_dict = torch.load(model_path)\n    keys = [k for k in state_dict.keys()]\n    filters = set()\n    filters_list = [state_dict[k].shape[0] for k in keys if not (state_dict[k].shape[0] in filters or filters.add(state_dict[k].shape[0]))]\n    res_blocks = len(set(k.split('.')[1] for k in state_dict.keys() if 'resnets' in k))\n    model = TransformerNet(filters=filters_list[:-1], res_blocks=res_blocks) \n    model.load_state_dict(state_dict, strict=False)\n    return model\n\ndef stylize(model_path: str, input_image: str, output_image: str, content_scale: float=None, \n            device: str=\"cpu\", export_onnx: bool=None):\n    \"\"\"Load a TransformerNet checkpoint, stylize an image and save the output\"\"\"\n    device = torch.device(device)\n    content_image = load_image(input_image, scale=content_scale)\n    content_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Lambda(lambda x: x.mul(255))\n    ])\n    content_image = content_transform(content_image)\n    content_image = content_image.unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        style_model = load_checkpoint(model_path)\n        style_model.to(device)\n         \n        if export_onnx:\n            assert export_onnx.endswith(\".onnx\"), \"Export model file should end with .onnx\"\n            output = torch.onnx._export(style_model, content_image, export_onnx, opset_version=9).cpu()\n        else:\n            output = style_model(content_image).cpu()\n    save_image(output_image, output[0])","metadata":{"id":"_6fV_AxBoMQl","executionInfo":{"status":"ok","timestamp":1635764425397,"user_tz":-120,"elapsed":1090,"user":{"displayName":"Augustas Masolas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBl4xvdqAPkl9CWBCfg6hmPeWdBpOaf2NVbWrMgQ=s64","userId":"10648435764537489448"}},"execution":{"iopub.status.busy":"2021-12-01T15:36:06.489754Z","iopub.execute_input":"2021-12-01T15:36:06.490082Z","iopub.status.idle":"2021-12-01T15:36:06.510185Z","shell.execute_reply.started":"2021-12-01T15:36:06.490022Z","shell.execute_reply":"2021-12-01T15:36:06.509251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define the Style Transfer Model\n\nHere we'll define the style transfer model itself. The model takes in an RGB image and generates a new image with the same dimensions. The features in the output image (e.g. color and texure) are then compared with the features of the style image and content image. The results of these comparisons are then used to update the parameters of the model so that it hopefully generates better images.\n\nI won't go into detail about the model architecture as the goal of this tutorial is primarily showing how to use it.","metadata":{"id":"7zTTy7Bmd4xf"}},{"cell_type":"code","source":"class TransformerNet(torch.nn.Module):\n    \"\"\"TransformerNet\n    https://github.com/pytorch/examples/blob/36441a83b6595524a538e342594ee6482754f374/fast_neural_style/neural_style/transformer_net.py#L4\n    \"\"\"\n    \n    def __init__(self, filters=(32, 64, 128), res_blocks=5):\n        super(TransformerNet, self).__init__()\n        self.filters = filters\n        self.res_blocks = res_blocks if res_blocks > 0 else 1\n        # Initial convolution layers\n        self.conv1 = ConvLayer(3, filters[0], kernel_size=9, stride=1)\n        self.in1 = torch.nn.InstanceNorm2d(filters[0], affine=True)\n        self.conv2 = ConvLayer(filters[0], filters[1], kernel_size=3, stride=2)\n        self.in2 = torch.nn.InstanceNorm2d(filters[1], affine=True)\n        self.conv3 = ConvLayer(filters[1], filters[2], kernel_size=3, stride=2)\n        self.in3 = torch.nn.InstanceNorm2d(filters[2], affine=True)\n        # Residual layers\n        self.resnets = torch.nn.ModuleList()\n        for i in range(self.res_blocks):\n            self.resnets.append(ResidualBlock(filters[2]))\n        \n        # Upsampling Layers\n        self.deconv1 = UpsampleConvLayer(filters[2], filters[1], kernel_size=3, stride=1, upsample=2)\n        self.in4 = torch.nn.InstanceNorm2d(filters[1], affine=True)\n        self.deconv2 = UpsampleConvLayer(filters[1], filters[0], kernel_size=3, stride=1, upsample=2)\n        self.in5 = torch.nn.InstanceNorm2d(filters[0], affine=True)\n        self.deconv3 = ConvLayer(filters[0], 3, kernel_size=9, stride=1)\n        # Non-linearities\n        self.relu = torch.nn.ReLU()\n        \n    def forward(self, X):\n        conv1_y = self.relu(self.in1(self.conv1(X)))\n        conv2_y = self.relu(self.in2(self.conv2(conv1_y)))\n        conv3_y = self.relu(self.in3(self.conv3(conv2_y)))\n\n        y = self.resnets[0](conv3_y) + conv3_y\n        \n        for i in range(1, self.res_blocks):\n            y = self.resnets[i](y) + y\n\n        y = self.relu(self.in4(self.deconv1(conv3_y + y)))\n        y = self.relu(self.in5(self.deconv2(conv2_y + y)))\n        y = self.deconv3(conv1_y + y)\n        return y\n\n\nclass ConvLayer(torch.nn.Module):\n    \"\"\"ConvLayer\n    https://github.com/pytorch/examples/blob/36441a83b6595524a538e342594ee6482754f374/fast_neural_style/neural_style/transformer_net.py#L44\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride):\n        super(ConvLayer, self).__init__()\n        reflection_padding = kernel_size // 2\n        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n\n    def forward(self, x):\n        out = self.reflection_pad(x)\n        out = self.conv2d(out)\n        return out\n\n\nclass ResidualBlock(torch.nn.Module):\n    \"\"\"ResidualBlock\n    introduced in: https://arxiv.org/abs/1512.03385\n    recommended architecture: http://torch.ch/blog/2016/02/04/resnets.html\n    https://github.com/pytorch/examples/blob/36441a83b6595524a538e342594ee6482754f374/fast_neural_style/neural_style/transformer_net.py#L57\n    \"\"\"\n\n    def __init__(self, channels):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n        self.in1 = torch.nn.InstanceNorm2d(channels, affine=True)\n        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n        self.in2 = torch.nn.InstanceNorm2d(channels, affine=True)\n        self.relu = torch.nn.ReLU()\n      \n    def forward(self, x):\n        residual = x\n        out = self.relu(self.in1(self.conv1(x)))\n        out = self.in2(self.conv2(out))\n        out = out + residual\n        return out\n\n\nclass UpsampleConvLayer(torch.nn.Module):\n    \"\"\"UpsampleConvLayer\n    Upsamples the input and then does a convolution. This method gives better results\n    compared to ConvTranspose2d.\n    ref: http://distill.pub/2016/deconv-checkerboard/\n    https://github.com/pytorch/examples/blob/36441a83b6595524a538e342594ee6482754f374/fast_neural_style/neural_style/transformer_net.py#L79\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n        super(UpsampleConvLayer, self).__init__()\n        self.upsample = upsample\n        reflection_padding = kernel_size // 2\n        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n        \n    def forward(self, x):\n        x_in = x\n        if self.upsample:\n            x_in = torch.nn.functional.interpolate(x_in, mode='nearest', scale_factor=self.upsample)\n        out = self.reflection_pad(x_in)\n        out = self.conv2d(out)\n        return out","metadata":{"id":"-qkcKSj3oxPQ","executionInfo":{"status":"ok","timestamp":1635764453691,"user_tz":-120,"elapsed":495,"user":{"displayName":"Augustas Masolas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBl4xvdqAPkl9CWBCfg6hmPeWdBpOaf2NVbWrMgQ=s64","userId":"10648435764537489448"}},"execution":{"iopub.status.busy":"2021-12-01T15:36:06.512297Z","iopub.execute_input":"2021-12-01T15:36:06.513155Z","iopub.status.idle":"2021-12-01T15:36:06.538603Z","shell.execute_reply.started":"2021-12-01T15:36:06.513118Z","shell.execute_reply":"2021-12-01T15:36:06.537764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define the VGG-19 Model\nNext, we'll define the model that will be used to judge the quality of the output images from the style transfer model. This model has been pretrained a large image dataset. This means it's already learned to recognize a wide variety of features in images. We'll use this model to extract the features of the content image, style image, and stylized images.","metadata":{"id":"LDlQpf-SAzN0"}},{"cell_type":"code","source":"class Vgg19(torch.nn.Module):\n    \"\"\"\n    https://github.com/pytorch/examples/blob/36441a83b6595524a538e342594ee6482754f374/fast_neural_style/neural_style/vgg.py#L7\n    \"\"\"\n    \n    def __init__(self, requires_grad=False):\n        super(Vgg19, self).__init__()\n        self.feature_layers = [0, 3, 5]\n        self.vgg_pretrained_features = vgg19(pretrained=True).features\n        self.slice1 = torch.nn.Sequential()\n        self.slice2 = torch.nn.Sequential()\n        self.slice3 = torch.nn.Sequential()\n        self.slice4 = torch.nn.Sequential()\n        self.slice5 = torch.nn.Sequential()\n        for x in range(4):\n            self.slice1.add_module(str(x), self.vgg_pretrained_features[x])\n        for x in range(4, 9):\n            self.slice2.add_module(str(x), self.vgg_pretrained_features[x])\n        for x in range(9, 18):\n            self.slice3.add_module(str(x), self.vgg_pretrained_features[x])\n        for x in range(18, 27):\n            self.slice4.add_module(str(x), self.vgg_pretrained_features[x])\n        for x in range(27, 36):\n            self.slice5.add_module(str(x), self.vgg_pretrained_features[x])\n        if not requires_grad:\n            for param in self.parameters():\n                param.requires_grad = False\n            \n    def forward(self, X):\n        h = self.slice1(X)\n        h_relu1_2 = h\n        h = self.slice2(h)\n        h_relu2_2 = h\n        h = self.slice3(h)\n        h_relu3_3 = h\n        h = self.slice4(h)\n        h_relu4_3 = h\n        h = self.slice5(h)\n        h_relu5_3 = h\n        vgg_outputs = namedtuple(\"VggOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])\n        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)\n        return out","metadata":{"id":"hEBFOVxrs6md","executionInfo":{"status":"ok","timestamp":1635764457139,"user_tz":-120,"elapsed":2,"user":{"displayName":"Augustas Masolas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBl4xvdqAPkl9CWBCfg6hmPeWdBpOaf2NVbWrMgQ=s64","userId":"10648435764537489448"}},"execution":{"iopub.status.busy":"2021-12-01T15:36:06.540967Z","iopub.execute_input":"2021-12-01T15:36:06.541272Z","iopub.status.idle":"2021-12-01T15:36:06.555281Z","shell.execute_reply.started":"2021-12-01T15:36:06.541248Z","shell.execute_reply":"2021-12-01T15:36:06.554476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define the Model Trainer\n\nWe'll define a new class to make training the style transfer model a bit easier. Along with training the model, this class will save the model's current progress at set intervals. It will also generate sample images so we can see how the model is doing. This will allow us to determine if the model is actually improving or whether it's already good enough that we can stop the training process early.","metadata":{"id":"DMSP4mAYoVHP"}},{"cell_type":"code","source":"class Trainer(object):\n    def __init__(self, train_loader, style_transform, generator, opt_generator, style_criterion, perception_model, device):\n        self.train_loader = train_loader\n        self.style_transform = style_transform\n        self.generator = generator\n        self.opt_generator = opt_generator\n        self.style_criterion = style_criterion\n        self.perception_model = perception_model\n        self.device = device\n        self.generator.to(self.device)\n        \n    def gram_matrix(self, y: torch.Tensor):\n        \"\"\"Compute the gram matrix a PyTorch Tensor\"\"\"\n        (b, ch, h, w) = y.size()\n        features = y.view(b, ch, w * h)\n        features_t = features.transpose(1, 2)\n        gram = features.bmm(features_t) / (ch * h * w)\n        return gram\n\n    def normalize_batch(self, batch: torch.Tensor):\n        \"\"\"Normalize a batch of Tensors using the imagenet mean and std \"\"\"\n        mean = batch.new_tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n        std = batch.new_tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n        batch = batch.div_(255.0)\n        return (batch - mean) / std\n\n    def get_gram_style(self, style_image: str, style_size: int):\n        \"\"\"Get the Gram Matrices for the style image\"\"\"\n        style = load_image(style_image, size=style_size)\n        style = self.style_transform(style)\n        style = style.repeat(self.train_loader.batch_size, 1, 1, 1).to(self.device)\n        features_style = self.perception_model(self.normalize_batch(style))\n        gram_style = [self.gram_matrix(y) for y in features_style]\n        return gram_style\n            \n    def save_checkpoint(self, path: str):\n        \"\"\"Save the current model weights at the specified path\"\"\"\n        self.generator.eval().cpu()\n        torch.save(self.generator.state_dict(), path)\n        print(f\"Checkpoint saved at {path}\")\n\n    def train(self, style_image, test_image, checkpoint_model_dir, epochs=5, content_weight=1e5, style_weight=1e10, \n                content_scale=None, style_size=None, log_interval=500, checkpoint_interval=500):\n        \"\"\"Train the style transfer model on the provided style image.\"\"\"\n        \n        gram_style = self.get_gram_style(style_image, style_size)\n\n        for e in range(epochs):\n            self.generator.train()\n            agg_content_loss = 0.\n            agg_style_loss = 0.\n            count = 0\n            for batch_id, (x, _) in enumerate(self.train_loader):\n                n_batch = len(x)\n                count += n_batch\n                self.opt_generator.zero_grad()\n                \n                x = x.to(self.device)\n                y = self.generator(x)\n\n                y = self.normalize_batch(y.clone())\n                x = self.normalize_batch(x.clone())\n                features_y = self.perception_model(y)\n                features_x = self.perception_model(x)\n\n                content_loss = content_weight * self.style_criterion(features_y.relu2_2, features_x.relu2_2)\n\n                style_loss = 0.\n                for ft_y, gm_s in zip(features_y, gram_style):\n                    gm_y = self.gram_matrix(ft_y)\n                    style_loss += self.style_criterion(gm_y, gm_s[:n_batch, :, :])\n                style_loss = style_loss * style_weight\n\n                total_loss = content_loss + style_loss\n                total_loss.backward()\n                self.opt_generator.step()\n\n                agg_content_loss += content_loss.item()\n                agg_style_loss += style_loss.item()\n\n                if (batch_id + 1) % log_interval == 0:\n                    mesg = f\"{' '.join(time.ctime().replace('  ', ' ').split(' ')[1:-1])}  \"\n                    mesg += f\"Epoch {e + 1}: [{count}/{len(self.train_loader.dataset)}]  \"\n                    mesg += f\"content: {(agg_content_loss / (batch_id + 1)):.4f}  \"\n                    mesg += f\"style: {(agg_style_loss / (batch_id + 1)):.4f}  \"\n                    mesg += f\"total: {((agg_content_loss + agg_style_loss) / (batch_id + 1)):.4f}\"\n                    print(mesg)\n\n                if checkpoint_model_dir is not None and (batch_id + 1) % checkpoint_interval == 0:\n                    ckpt_base = f\"ckpt_epoch_{e}_batch_id_{batch_id + 1}\"\n                    ckpt_model_filename = ckpt_base + \".pth\"\n                    ckpt_model_path = os.path.join(checkpoint_model_dir, ckpt_model_filename)\n                    self.save_checkpoint(ckpt_model_path)\n                    output_image = ckpt_base + \".png\"\n                    output_image_path = os.path.join(checkpoint_model_dir, output_image)\n                    stylize(ckpt_model_path, test_image, output_image_path)\n                    self.generator.to(self.device).train()\n                \n        print(\"Finished Training\")\n        ckpt_model_path = os.path.join(checkpoint_model_dir, 'final.pth')\n        self.save_checkpoint(ckpt_model_path)\n        output_image_path = os.path.join(checkpoint_model_dir, 'final.png')\n        stylize(ckpt_model_path, test_image, output_image_path)","metadata":{"id":"kbwOOyEloAiu","executionInfo":{"status":"ok","timestamp":1635764462569,"user_tz":-120,"elapsed":1286,"user":{"displayName":"Augustas Masolas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBl4xvdqAPkl9CWBCfg6hmPeWdBpOaf2NVbWrMgQ=s64","userId":"10648435764537489448"}},"execution":{"iopub.status.busy":"2021-12-01T15:36:06.556852Z","iopub.execute_input":"2021-12-01T15:36:06.55744Z","iopub.status.idle":"2021-12-01T15:36:06.582668Z","shell.execute_reply.started":"2021-12-01T15:36:06.557404Z","shell.execute_reply":"2021-12-01T15:36:06.581871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"style_images_dir = f\"../input/kursinisdataset/style_images\"\ntest_images_dir = f\"../input/kursinisdataset/test_images\"\ncheckpoints_dir = f\"/checkpoints\"\nmake_dir(checkpoints_dir)","metadata":{"id":"oSXvYLvFcQN6","executionInfo":{"status":"ok","timestamp":1635764430435,"user_tz":-120,"elapsed":7,"user":{"displayName":"Augustas Masolas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBl4xvdqAPkl9CWBCfg6hmPeWdBpOaf2NVbWrMgQ=s64","userId":"10648435764537489448"}},"outputId":"be6874e2-d3cd-4b0a-ec71-fd69fab15533","execution":{"iopub.status.busy":"2021-12-01T15:36:06.584055Z","iopub.execute_input":"2021-12-01T15:36:06.584669Z","iopub.status.idle":"2021-12-01T15:36:06.597249Z","shell.execute_reply.started":"2021-12-01T15:36:06.584635Z","shell.execute_reply":"2021-12-01T15:36:06.59628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!mkdir ./video_frames/","metadata":{"execution":{"iopub.status.busy":"2021-12-01T15:36:06.598641Z","iopub.execute_input":"2021-12-01T15:36:06.599186Z","iopub.status.idle":"2021-12-01T15:36:06.605002Z","shell.execute_reply.started":"2021-12-01T15:36:06.599147Z","shell.execute_reply":"2021-12-01T15:36:06.603977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!ffmpeg -i ../input/unityviking-gamefootage/movie_002.mp4 ./video_frames/%05d.png -hide_banner","metadata":{"execution":{"iopub.status.busy":"2021-12-01T15:36:06.607908Z","iopub.execute_input":"2021-12-01T15:36:06.608584Z","iopub.status.idle":"2021-12-01T15:36:06.614787Z","shell.execute_reply.started":"2021-12-01T15:36:06.60852Z","shell.execute_reply":"2021-12-01T15:36:06.613778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create the Trainer Variables\n\nIn this section we'll define the variables required to define a new Trainer.","metadata":{"id":"u4kDxfZ2nq3s"}},{"cell_type":"markdown","source":"### Define the `DataLoader`\nWe need to define a `DataLoader` that will be responsible for iterating through the dataset during training.\n\nWe also need to specify the `batch_size` which indicates how many images will be fed to the model at a time.\n\nEvery image in a batch needs to be the same size. We'll set the size using the `image_size` variable.\n\nImages need to be processed before being fed to the model. We'll define the preprocessing steps using the `transforms.Compose()` method. Our preprocessing steps include the following:\n\n1. Resize the images in the current batch to the target `image_size`\n2. Crop the images so that they are all square\n3. Convert the images to PyTorch Tensors\n4. Multiply the color channel values by 255\n\nWe then store the list of images in the `dataset_dir` along with the preprocessing steps in a new variable called `train_dataset`.\n\nFinally, we create our `DataLoader` using the `train_dataset` and specified `batch_size`","metadata":{"id":"D_ORbpavspLM"}},{"cell_type":"code","source":"batch_size = 4\nimage_size = 256\ntransform = transforms.Compose([transforms.Resize(image_size),\n                                transforms.CenterCrop(image_size),\n                                transforms.ToTensor(),\n                                transforms.Lambda(lambda x: x.mul(255))\n                                ])\n\nl = []\nl.append(ImageFolder('../input/image-dataset', transform))\nl.append(ImageFolder('../input/unityviking-gamefootage/video_frames', transform))\nimage_datasets = torch.utils.data.ConcatDataset(l)\nprint(l)\nprint(image_datasets)\n\ntrain_dataset = image_datasets\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size)","metadata":{"id":"3CU3V_Rpsqr_","execution":{"iopub.status.busy":"2021-12-01T15:36:06.616446Z","iopub.execute_input":"2021-12-01T15:36:06.616957Z","iopub.status.idle":"2021-12-01T15:37:07.163331Z","shell.execute_reply.started":"2021-12-01T15:36:06.616916Z","shell.execute_reply":"2021-12-01T15:37:07.162452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Select Compute Device\n\nWe'll double check that a cuda GPU is available using the `torch.cuda.is_available()` method.","metadata":{"id":"fNIciP5Cn0Th"}},{"cell_type":"code","source":"use_cuda = True\ndevice = \"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\"\nprint(f\"Using: {device}\")","metadata":{"id":"lap1HXkUuAwJ","executionInfo":{"status":"ok","timestamp":1635697692650,"user_tz":-120,"elapsed":6,"user":{"displayName":"Augustas Masolas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBl4xvdqAPkl9CWBCfg6hmPeWdBpOaf2NVbWrMgQ=s64","userId":"10648435764537489448"}},"outputId":"bec09148-0989-4518-ce8f-881fa353aa37","execution":{"iopub.status.busy":"2021-12-01T15:37:07.164825Z","iopub.execute_input":"2021-12-01T15:37:07.165603Z","iopub.status.idle":"2021-12-01T15:37:07.172206Z","shell.execute_reply.started":"2021-12-01T15:37:07.165551Z","shell.execute_reply":"2021-12-01T15:37:07.171406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Transforms for Style Image\n\nNext we'll define the transforms used to process the style image before feeding it to the VGG-19 model. The processing steps are basically the same as for the training images except the style image will have already been resized.\n\n1. Convert the image to a PyTorch Tensor\n2. Multiply the pixel values by 255","metadata":{"id":"HnXBUPXQojSH"}},{"cell_type":"code","source":"style_transform = transforms.Compose([transforms.ToTensor(),\n                                      transforms.Lambda(lambda x: x.mul(255))\n                                      ])","metadata":{"id":"rKiv1F9vv24k","execution":{"iopub.status.busy":"2021-12-01T15:37:07.173616Z","iopub.execute_input":"2021-12-01T15:37:07.174334Z","iopub.status.idle":"2021-12-01T15:37:07.186255Z","shell.execute_reply.started":"2021-12-01T15:37:07.174295Z","shell.execute_reply":"2021-12-01T15:37:07.185393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create the Style Transfer Model\n\nNext, we'll create a new instance of the style transfer model. It's here that you'll be able to experiment with tradeoffs between performance and quality.\n\n#### Tuning Model Inference Speed:\nThe easiest way to make the style transfer model faster is to make it smaller. We can easily tune the size of the model by adjusting the size of the layers or by using fewer layers.\n\n\n##### Resolution: `960x540`\n##### Filters: `(16, 32, 64)`\n```\n================================================================\nTotal params: 424,899\nTrainable params: 424,899\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 5.93\nForward/backward pass size (MB): 2210.61\nParams size (MB): 1.62\nEstimated Total Size (MB): 2218.17\n----------------------------------------------------------------\n```\n\n##### Resolution: `960x540`\n##### Filters: `(32, 64, 128)`\n```\n================================================================\nTotal params: 1,679,235\nTrainable params: 1,679,235\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 5.93\nForward/backward pass size (MB): 4385.35\nParams size (MB): 6.41\nEstimated Total Size (MB): 4397.69\n----------------------------------------------------------------\n```\n\nBy default, the style transfer model uses the following values:\n* filters: (32, 64, 128)\n* res_blocks: 5\n\nThe `filters` variable determines the size of the layers in the model. The `res_blocks` variable determines the number of `ResidualBlocks` that form the core of the model.\n\nI've found that setting filters to `(8, 16, 32)` and keeping res_blocks at `5` significantly improves performance in Unity with minimal impact on quality.","metadata":{"id":"a2U0SBS1pNFp"}},{"cell_type":"code","source":"filters = (12, 24, 48)\nres_blocks = 5\ngenerator = TransformerNet(filters=filters, res_blocks=res_blocks).to(device)","metadata":{"id":"XiNEW2rvoxCK","execution":{"iopub.status.busy":"2021-12-01T15:37:07.187805Z","iopub.execute_input":"2021-12-01T15:37:07.188518Z","iopub.status.idle":"2021-12-01T15:37:07.209903Z","shell.execute_reply.started":"2021-12-01T15:37:07.188478Z","shell.execute_reply":"2021-12-01T15:37:07.209131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = 1e-3\nopt_generator = torch.optim.Adam(generator.parameters(), lr)","metadata":{"id":"SResZ2txpZOf","execution":{"iopub.status.busy":"2021-12-01T15:37:07.211061Z","iopub.execute_input":"2021-12-01T15:37:07.211567Z","iopub.status.idle":"2021-12-01T15:37:07.217489Z","shell.execute_reply.started":"2021-12-01T15:37:07.211521Z","shell.execute_reply":"2021-12-01T15:37:07.216449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define How Model Performance Will Be Measured\n\nWe'll be using Mean Squared Error (MSE) for comparing the difference between the features of the content image and stylized image and between the features of the stylized image and the target style image.","metadata":{"id":"TA645RtxAzN9"}},{"cell_type":"code","source":"style_criterion = torch.nn.MSELoss()","metadata":{"id":"y1O6wWIhcqXR","execution":{"iopub.status.busy":"2021-12-01T15:37:07.218938Z","iopub.execute_input":"2021-12-01T15:37:07.219621Z","iopub.status.idle":"2021-12-01T15:37:07.226035Z","shell.execute_reply.started":"2021-12-01T15:37:07.219583Z","shell.execute_reply":"2021-12-01T15:37:07.225131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create a New VGG-19 Perception Model\n\nNext, we'll create a new vgg-19 model. The pretrained model will be downloaded the first time this cell is run.","metadata":{"id":"Jj6Z9ToEAzN-"}},{"cell_type":"code","source":"perception_model = Vgg19(requires_grad=False).to(device)","metadata":{"id":"DV5DOst3s6ZD","executionInfo":{"status":"ok","timestamp":1635697709838,"user_tz":-120,"elapsed":8505,"user":{"displayName":"Augustas Masolas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBl4xvdqAPkl9CWBCfg6hmPeWdBpOaf2NVbWrMgQ=s64","userId":"10648435764537489448"}},"outputId":"0a5e5bfa-5798-4457-de48-60344da98373","execution":{"iopub.status.busy":"2021-12-01T15:37:07.227573Z","iopub.execute_input":"2021-12-01T15:37:07.22823Z","iopub.status.idle":"2021-12-01T15:37:08.738565Z","shell.execute_reply.started":"2021-12-01T15:37:07.228193Z","shell.execute_reply":"2021-12-01T15:37:08.737693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create a New Trainer\n\nWe can now create a new trainer instance using the variables we defined above.","metadata":{"id":"nxVg-Eq0uBCW"}},{"cell_type":"code","source":"trainer = Trainer(train_loader=train_loader, \n                  style_transform=style_transform, \n                  generator=generator, \n                  opt_generator=opt_generator, \n                  style_criterion=style_criterion, \n                  perception_model=perception_model, \n                  device=device)","metadata":{"id":"eHajCWW0cqdu","execution":{"iopub.status.busy":"2021-12-01T15:37:08.739916Z","iopub.execute_input":"2021-12-01T15:37:08.74028Z","iopub.status.idle":"2021-12-01T15:37:08.748698Z","shell.execute_reply.started":"2021-12-01T15:37:08.740242Z","shell.execute_reply":"2021-12-01T15:37:08.747638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Tuning the Stylized Image\n\nThe stylized image will be influenced by the following:\n\n* Influence of the content image\n* Influence of the style image\n* Size of the style image\n\nI recommend keeping the content_weight at `1e5` and adjusting the style_weight between `5e8` and `1e11`. The ideal style_weight will vary depending on the style image. I recommend starting out low, training for 5-10 checkpoint intervals, and increasing the style weight as needed.","metadata":{"id":"TUDg6xkDAzN_"}},{"cell_type":"code","source":"# The file path for the target style image\nstyle_image = f\"../input/kursinisdataset/style_images/fire.jpg\"\n\n# The file path for a sample input image for demonstrating the model's progress during training\ntest_image = f\"{test_images_dir}/011.png\" \n\n# The number of times to iterate through the entire training dataset\nepochs = 5\n\n# The influence from the input image on the stylized image\n# Default: 1e5\ncontent_weight = 1e5\n# The influence from the style image on the stylized image\n# Default: 1e10\nstyle_weight = 1e10\n\n# (test_image resolution) / content_scale\n# Default: 1.0\ncontent_scale = 0.8\n# Target size for style_image = (style_size, styl_size)\n# Default: 256\nstyle_size = 720\n\n# The number of training batches to wait before printing the progress of the model \nlog_interval = 500\n# The number of training to wait before saving the current model weights\ncheckpoint_interval = 500","metadata":{"id":"Ne7Xy_r_cpqA","execution":{"iopub.status.busy":"2021-12-01T15:39:12.161834Z","iopub.execute_input":"2021-12-01T15:39:12.162162Z","iopub.status.idle":"2021-12-01T15:39:12.166996Z","shell.execute_reply.started":"2021-12-01T15:39:12.162131Z","shell.execute_reply":"2021-12-01T15:39:12.166139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Train the Model","metadata":{"id":"CD6BP_Nx1em6"}},{"cell_type":"code","source":"trainer.train(style_image=style_image, \n              test_image=test_image, \n              checkpoint_model_dir=checkpoints_dir, \n              epochs=epochs, \n              content_weight=content_weight, \n              style_weight=style_weight,\n              content_scale=content_scale,\n              style_size=style_size,\n              log_interval=log_interval, \n              checkpoint_interval=checkpoint_interval)","metadata":{"id":"z18R1u54cqim","outputId":"a82c368f-31af-4c81-c24f-f83344e4c5da","execution":{"iopub.status.busy":"2021-12-01T15:39:14.297499Z","iopub.execute_input":"2021-12-01T15:39:14.297825Z","iopub.status.idle":"2021-12-01T15:42:59.90327Z","shell.execute_reply.started":"2021-12-01T15:39:14.297796Z","shell.execute_reply":"2021-12-01T15:42:59.900752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Export the model to ONNX\nWe can finally export the model to ONNX format. PyTorch exports models by feeding a sample input into the model and tracing what operators are used to compute the outputs.\n\nWe'll use a `(1, 3, 960, 540)` Tensor with random values as our sample input. This is equivalent to feeding a `960x540` RGB image to the model. The resolution doesn't matter as we can feed images with arbitrary resolutions once the model is exported.\n\n**Note:** You will get a warning after running the code cell below recommending that you use ONNX opset 11 or above. Unity has prioritized support for opset 9 for Barracuda and higher opsets are not fully supported.","metadata":{"id":"jdYeHD4Kg_MB"}},{"cell_type":"code","source":"checkpoint_path = f\"{checkpoints_dir}/final.pth\"\nstyle_model = load_checkpoint(checkpoint_path)","metadata":{"id":"hUK6RDfSwXxE","executionInfo":{"status":"ok","timestamp":1635764466804,"user_tz":-120,"elapsed":495,"user":{"displayName":"Augustas Masolas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBl4xvdqAPkl9CWBCfg6hmPeWdBpOaf2NVbWrMgQ=s64","userId":"10648435764537489448"}},"execution":{"iopub.status.busy":"2021-12-01T15:37:08.932415Z","iopub.status.idle":"2021-12-01T15:37:08.933348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = torch.randn(1, 3, 960, 540).cpu()\n\ntorch.onnx.export(style_model.cpu(),     #  Model being run\n                  x,                           # Sample input\n                  f\"fnlAdditionalTrainData.onnx\", # Path to save ONNX file\n                  export_params=True,          # Store trained weights\n                  opset_version=9,             # Which ONNX version to use\n                  do_constant_folding=True     # Replace operations that have all constant inputs with pre-computed nodes\n                 )","metadata":{"id":"ZGOvUXiGH08w","executionInfo":{"status":"ok","timestamp":1635764472221,"user_tz":-120,"elapsed":1752,"user":{"displayName":"Augustas Masolas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBl4xvdqAPkl9CWBCfg6hmPeWdBpOaf2NVbWrMgQ=s64","userId":"10648435764537489448"}},"outputId":"936995bd-64c1-4915-c733-844f7f6b1e7e","execution":{"iopub.status.busy":"2021-12-01T15:37:08.934778Z","iopub.status.idle":"2021-12-01T15:37:08.935625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"xdJO0yRsH0oq"},"execution_count":null,"outputs":[]}]}